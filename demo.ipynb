{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 添加依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 添加依赖，主要是总依赖，包含EfficientFaceTemporal的初始化\n",
    "import time\n",
    "import math\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "from numpy.lib.function_base import _quantile_unchecked\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.backends import cudnn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "# matplotlib.use('TkAgg')\n",
    "\n",
    "from models import multimodalcnn\n",
    "import utils\n",
    "\n",
    "from models.modulator import Modulator\n",
    "from models.efficientface import LocalFeatureExtractor, InvertedResidual\n",
    "from models.transformer_timm import AttentionBlock, Attention\n",
    "import torchvision.models as models\n",
    "\n",
    "def conv1d_block(in_channels, out_channels, kernel_size=3, stride=1, padding='same'):\n",
    "    return nn.Sequential(nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size,stride=stride, padding=padding),nn.BatchNorm1d(out_channels),\n",
    "                                   nn.ReLU(inplace=True)) \n",
    "\n",
    "class EfficientFaceTemporal(nn.Module):\n",
    "\n",
    "    def __init__(self, stages_repeats, stages_out_channels, num_classes=7, im_per_sample=25):\n",
    "        super(EfficientFaceTemporal, self).__init__()\n",
    "\n",
    "        if len(stages_repeats) != 3:\n",
    "            raise ValueError('expected stages_repeats as list of 3 positive ints')\n",
    "        if len(stages_out_channels) != 5:\n",
    "            raise ValueError('expected stages_out_channels as list of 5 positive ints')\n",
    "        self._stage_out_channels = stages_out_channels\n",
    "\n",
    "        input_channels = 3\n",
    "        output_channels = self._stage_out_channels[0]\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(input_channels, output_channels, 3, 2, 1, bias=False),\n",
    "                                   nn.BatchNorm2d(output_channels),\n",
    "                                   nn.ReLU(inplace=True),)\n",
    "        input_channels = output_channels\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        stage_names = ['stage{}'.format(i) for i in [2, 3, 4]]\n",
    "        for name, repeats, output_channels in zip(stage_names, stages_repeats, self._stage_out_channels[1:]):\n",
    "            seq = [InvertedResidual(input_channels, output_channels, 2)]\n",
    "            for i in range(repeats - 1):\n",
    "                seq.append(InvertedResidual(output_channels, output_channels, 1))\n",
    "            setattr(self, name, nn.Sequential(*seq))\n",
    "            input_channels = output_channels\n",
    "\n",
    "        self.local = LocalFeatureExtractor(29, 116, 1)\n",
    "        self.modulator = Modulator(116)\n",
    "\n",
    "        output_channels = self._stage_out_channels[-1]\n",
    "\n",
    "        self.conv5 = nn.Sequential(nn.Conv2d(input_channels, output_channels, 1, 1, 0, bias=False),\n",
    "                                   nn.BatchNorm2d(output_channels),\n",
    "                                   nn.ReLU(inplace=True),)\n",
    "        self.conv1d_0 = conv1d_block(output_channels, 64)\n",
    "        self.conv1d_1 = conv1d_block(64, 64)\n",
    "        self.conv1d_2 = conv1d_block(64, 128)\n",
    "        self.conv1d_3 = conv1d_block(128, 128)\n",
    "\n",
    "        # self.resnet50 = models.resnet50(pretrained=False)\n",
    "        self.resnet50 = models.resnet50(pretrained=True)\n",
    "        self.adaptive_avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "        # self.resnet_conv1 =    resnet50.conv1\n",
    "        # self.resnet_bn1 =      resnet50.bn1\n",
    "        # self.resnet_relu =     resnet50.relu\n",
    "        # self.resnet_maxpool =  resnet50.maxpool\n",
    "        # self.resnet_layer1 =   resnet50.layer1\n",
    "        # self.resnet_layer2 =   resnet50.layer2\n",
    "        # self.resnet_layer3 =   resnet50.layer3\n",
    "        # self.resnet_layer4 =   resnet50.layer4\n",
    "        # self.resnet_avgpool =  resnet50.avgpool\n",
    "\n",
    "        self.classifier_1 = nn.Sequential(\n",
    "                nn.Linear(128, num_classes),\n",
    "            )\n",
    "        self.im_per_sample = im_per_sample\n",
    "        \n",
    "    def forward_features(self, x):  # torch.Size([1200, 3, 224, 224])\n",
    "        x = self.conv1(x)   # torch.Size([1200, 29, 112, 112])\n",
    "        x = self.maxpool(x) # torch.Size([1200, 29, 56, 56])\n",
    "        x = self.modulator(self.stage2(x)) + self.local(x)  # torch.Size([1200, 116, 28, 28])\n",
    "        x = self.stage3(x)  # torch.Size([1200, 232, 14, 14])\n",
    "        x = self.stage4(x)  # torch.Size([1200, 464, 7, 7])\n",
    "        x = self.conv5(x)   # torch.Size([1200, 1024, 7, 7])\n",
    "        # 对每个通道上的所有元素求平均值。这样就得到了一个一维向量作为输出\n",
    "        x = x.mean([2, 3]) #global average pooling， torch.Size([1200, 1024])\n",
    "        return x\n",
    "        \n",
    "    def forward_features_resnet(self, x):  # torch.Size([1200, 3, 224, 224])\n",
    "        x = self.resnet50.conv1(x)\n",
    "        x = self.resnet50.bn1(x)\n",
    "        x = self.resnet50.relu(x)\n",
    "        x = self.resnet50.maxpool(x)\n",
    "        x = self.resnet50.layer1(x)\n",
    "        x = self.resnet50.layer2(x)\n",
    "        x = self.resnet50.layer3(x)\n",
    "\n",
    "        x = self.adaptive_avgpool(x)\n",
    "        # x = torch.randn(1200, 1024, 7, 7)\n",
    "        # 对每个通道上的所有元素求平均值。这样就得到了一个一维向量作为输出\n",
    "        x = x.mean([2, 3]) #global average pooling， torch.Size([1200, 1024])\n",
    "        return x\n",
    "\n",
    "    def forward_stage1(self, x):\n",
    "        #Getting samples per batch\n",
    "        assert x.shape[0] % self.im_per_sample == 0, \"Batch size is not a multiple of sequence length.\"\n",
    "        n_samples = x.shape[0] // self.im_per_sample\n",
    "        x = x.view(n_samples, self.im_per_sample, x.shape[1])\n",
    "        x = x.permute(0,2,1)\n",
    "        x = self.conv1d_0(x)\n",
    "        x = self.conv1d_1(x)\n",
    "        return x\n",
    "        \n",
    "        \n",
    "    def forward_stage2(self, x):\n",
    "        x = self.conv1d_2(x)\n",
    "        x = self.conv1d_3(x)\n",
    "        return x\n",
    "    \n",
    "    def forward_classifier(self, x):\n",
    "        x = x.mean([-1]) #pooling accross temporal dimension\n",
    "        x1 = self.classifier_1(x)\n",
    "        return x1\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.forward_stage1(x)\n",
    "        x = self.forward_stage2(x)\n",
    "        x = self.forward_classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 处理图片序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.conda/envs/multimodal-emotion-recognition/lib/python3.9/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "# 处理图片序列，（可以简单认为是处理视频），处理完应该主要得到的是numpy_video\n",
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import numpy as np          \n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from facenet_pytorch import MTCNN\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "mtcnn = MTCNN(image_size=(720, 1280), device=device)\n",
    "\n",
    "#mtcnn.to(device)\n",
    "save_frames = 15\n",
    "input_fps = 30\n",
    "\n",
    "save_length = 3.6 #seconds\n",
    "save_avi = False # True\n",
    "\n",
    "failed_videos = []\n",
    "# root = '/lustre/scratch/chumache/RAVDESS_or/'\n",
    "root = '/home/ubuntu/work_space/datasets/RAVDESS_autido_speech/Actor_03/'\n",
    "\n",
    "# 这段代码定义了一个lambda函数select_distributed，\n",
    "# 它的作用是将视频的帧数分成若干段，然后在每一段中均匀地选择一些帧。\n",
    "# 具体来说，它接受两个参数m和n，其中m表示要分成的段数，n表示视频的总帧数。\n",
    "# 它返回一个长度为m的列表，列表中的每个元素表示在对应的段中选择的帧的索引。\n",
    "# 这个函数在后面的代码中被用来选择视频中的一些帧进行人脸检测和裁剪。\n",
    "select_distributed = lambda m, n: [i*n//m + n//(2*m) for i in range(m)]\n",
    "n_processed = 0\n",
    "filename = root + \"01-01-03-02-01-01-03.mp4\"\n",
    "           \n",
    "if filename.endswith('.mp4'):\n",
    "                \n",
    "    cap = cv2.VideoCapture(filename)\n",
    "    #calculate length in frames\n",
    "    framen = 0\n",
    "    while True:\n",
    "        i,q = cap.read()\n",
    "        if not i:\n",
    "            break\n",
    "        framen += 1\n",
    "    cap = cv2.VideoCapture(filename)\n",
    "\n",
    "    # 这几行代码中的变量save_length表示要保存的视频长度(秒)，\n",
    "    # input_fps表示视频的帧率，save_frames表示要保存的帧数，mtcnn是MTCNN模型的实例。\n",
    "    # 如果视频的帧数小于要保存的帧数，代码会跳过一些帧以确保保存的帧数正确。\n",
    "    # 如果视频处理失败，代码会将其添加到failed_videos列表中。\n",
    "    \n",
    "    # 这段代码实际上相当于if save_length > framen / input_fps:\n",
    "    # 也就是说视频总时长小于需要保存的时长，这时候为什么要跳过帧呢？\n",
    "    # 按理说不该是小于的时候才去掉两边吗，我感觉这里应该是写反了\n",
    "    # 考虑save_length*input_fps = 3 * framen，这时候下面完全不会裁剪视频\n",
    "    if save_length*input_fps > framen:                    \n",
    "        skip_begin = int((framen - (save_length*input_fps)) // 2)\n",
    "        for i in range(skip_begin):\n",
    "            # 跳过一些帧，读取但是不处理就是跳过了\n",
    "            _, im = cap.read() \n",
    "            \n",
    "    framen = int(save_length*input_fps)    \n",
    "    frames_to_select = select_distributed(save_frames,framen)\n",
    "    save_fps = save_frames // (framen // input_fps) \n",
    "    if save_avi:\n",
    "        out = cv2.VideoWriter(filename[:-4]+'_facecroppad.avi',cv2.VideoWriter_fourcc('M','J','P','G'), save_fps, (224,224))\n",
    "\n",
    "    numpy_video = []\n",
    "    success = 0\n",
    "    frame_ctr = 0\n",
    "    \n",
    "    while True: \n",
    "        ret, im = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if frame_ctr not in frames_to_select:\n",
    "            frame_ctr += 1\n",
    "            continue\n",
    "        else:\n",
    "            frames_to_select.remove(frame_ctr)\n",
    "            frame_ctr += 1\n",
    "\n",
    "        try:\n",
    "            gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "        except:\n",
    "            failed_videos.append(i)\n",
    "            break\n",
    "\n",
    "        temp = im[:,:,-1]\n",
    "        im_rgb = im.copy()\n",
    "        im_rgb[:,:,-1] = im_rgb[:,:,0]\n",
    "        im_rgb[:,:,0] = temp\n",
    "        im_rgb = torch.tensor(im_rgb)\n",
    "        im_rgb = im_rgb.to(device)\n",
    "\n",
    "        bbox = mtcnn.detect(im_rgb)\n",
    "        if bbox[0] is not None:\n",
    "            bbox = bbox[0][0]\n",
    "            bbox = [round(x) for x in bbox]\n",
    "            x1, y1, x2, y2 = bbox\n",
    "        im = im[y1:y2, x1:x2, :]\n",
    "        im = cv2.resize(im, (224,224))\n",
    "        if save_avi:\n",
    "            out.write(im)\n",
    "        numpy_video.append(im)\n",
    "    # 如果可以添加的帧不足，则用空白帧替换\n",
    "    if len(frames_to_select) > 0:\n",
    "        for i in range(len(frames_to_select)):\n",
    "            if save_avi:\n",
    "                out.write(np.zeros((224,224,3), dtype = np.uint8))\n",
    "            numpy_video.append(np.zeros((224,224,3), dtype=np.uint8))\n",
    "    if save_avi:\n",
    "        out.release() \n",
    "    np.save(filename[:-4]+'_facecroppad.npy', np.array(numpy_video))\n",
    "    if len(numpy_video) != 15:\n",
    "        print('Error', filename)    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 处理音频序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.conda/envs/multimodal-emotion-recognition/lib/python3.9/site-packages/librosa/core/audio.py:165: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n"
     ]
    }
   ],
   "source": [
    "# 处理音频序列，这里的主要输出应该是y和sr\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import librosa\n",
    "import os\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "\n",
    "#audiofile = 'E://OpenDR_datasets//RAVDESS//Actor_19//03-01-07-02-01-02-19.wav'\n",
    "##this file preprocess audio files to ensure they are of the same length. if length is less than 3.6 seconds, it is padded with zeros in the end. otherwise, it is equally cropped from \n",
    "##both sides\n",
    "\n",
    "# root = '/lustre/scratch/chumache/RAVDESS_or/'\n",
    "root = '/home/ubuntu/work_space/datasets/RAVDESS_autido_speech'\n",
    "target_time = 3.6 #sec\n",
    "audiofile = filename\n",
    "        \n",
    "# if not audiofile.endswith('.wav') or 'croppad' in audiofile:\n",
    "#     continue\n",
    "\n",
    "audios = librosa.core.load(audiofile, sr=22050)\n",
    "\n",
    "y = audios[0]\n",
    "sr = audios[1]\n",
    "target_length = int(sr * target_time)\n",
    "if len(y) < target_length:\n",
    "    y = np.array(list(y) + [0 for i in range(target_length - len(y))])\n",
    "else:\n",
    "    remain = len(y) - target_length\n",
    "    y = y[remain//2:-(remain - remain//2)]\n",
    "\n",
    "# sf.write(audiofile[:-4]+'_croppad.wav', y, sr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 获取模型输出结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing efficientnet\n",
      "<class 'models.multimodalcnn.MultiModalCNN'>\n"
     ]
    }
   ],
   "source": [
    "# 加载模型\n",
    "num_classes = 8\n",
    "seq_length = 15\n",
    "pretrain_state_path = '/home/ubuntu/work_space/multimodal-emotion-recognition-experiment/best_results/1678717134.4151022lr_0.00017341600515462103seed_42optimizer_AdamWweight_decay_0.001_best_complete/RAVDESS_multimodalcnn_15_best0.pth'\n",
    "pretrain_path = '/home/ubuntu/work_space/EfficientFace-master/checkpoint/Pretrained_EfficientFace.tar'\n",
    "model = multimodalcnn.MultiModalCNN(fusion = 'iaLSTM', pretr_ef = pretrain_path)\n",
    "pretrained_state = torch.load(pretrain_state_path)\n",
    "pretrained_state_dict = pretrained_state['state_dict']\n",
    "# 这里要将一些字符串替换掉才能得到合适的字典\n",
    "pretrained_state_dict = {key.replace(\"module.\", \"\"): value for key, value in pretrained_state_dict.items()}\n",
    "model.load_state_dict(pretrained_state_dict)\n",
    "\n",
    "print(type(model))\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理音视频数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([15, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([10, 156])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 10, 156])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([64, 10, 3])\n"
     ]
    }
   ],
   "source": [
    "# 处理音视频数据\n",
    "import transforms\n",
    "video_transform = transforms.Compose([\n",
    "                transforms.ToTensor(255)])\n",
    "# video_transform = transforms.Compose([\n",
    "#     transforms.ToTensor()]) # opt.video_norm_value\n",
    "# test_data = get_test_set(opt, spatial_transform=video_transform)\n",
    "def video_loader(video_dir_path):\n",
    "    video = np.load(video_dir_path)    \n",
    "    video_data = []\n",
    "    for i in range(np.shape(video)[0]):\n",
    "        video_data.append(Image.fromarray(video[i,:,:,:]))    \n",
    "    return video_data\n",
    "\n",
    "import functools\n",
    "def get_default_video_loader():\n",
    "    return functools.partial(video_loader)\n",
    "\n",
    "self_loader = get_default_video_loader()\n",
    "video_path = '/home/ubuntu/work_space/datasets/RAVDESS_autido_speech/Actor_20/02-01-03-01-02-02-20_facecroppad.npy'\n",
    "clip = self_loader(video_path)\n",
    "self_spatial_transform = video_transform\n",
    "self_spatial_transform.randomize_parameters()\n",
    "# 这一句就让之前的视频加载操作无效了\n",
    "clip = numpy_video\n",
    "clip = [self_spatial_transform(img) for img in clip]\n",
    "clip = torch.stack(clip, 0)\n",
    "print(type(clip))\n",
    "print(clip.shape)\n",
    "\n",
    "import librosa\n",
    "# def load_audio(audiofile, sr):\n",
    "#     audios = librosa.core.load(audiofile, sr)\n",
    "#     y = audios[0]\n",
    "#     return y, sr\n",
    "# audio_path = '/home/ubuntu/work_space/datasets/RAVDESS_autido_speech/Actor_20/03-01-03-01-02-02-20_croppad.wav'\n",
    "# y, sr = load_audio(audio_path, sr=22050)\n",
    "\n",
    "def get_mfccs(y, sr):\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=10)\n",
    "    return mfcc\n",
    "audio_features = get_mfccs(y, sr)\n",
    "\n",
    "audio_features = torch.from_numpy(audio_features)\n",
    "print(type(audio_features))\n",
    "print(audio_features.shape)\n",
    "\n",
    "# 将 audio_features 张量的维度从 (batch_size, num_channels, sequence_length) \n",
    "# 扩展为 (1, batch_size, num_channels, sequence_length)。\n",
    "audio_features = audio_features.unsqueeze(0)\n",
    "print(type(audio_features))\n",
    "print(audio_features.shape)\n",
    "\n",
    "audio_features_tmp = torch.zeros((64, 10, 3))\n",
    "print(type(audio_features_tmp))\n",
    "print(audio_features_tmp.shape)\n",
    "\n",
    "# audio_features and clip is next input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4506, 0.7349, 0.7496, 0.2442, 0.2085, 0.6537, 0.4141, 0.8010,\n",
      "          0.6745, 0.9483, 0.3453, 0.5775],\n",
      "         [0.5649, 0.2819, 0.6138, 0.2782, 0.0668, 0.2279, 0.3788, 0.2516,\n",
      "          0.7972, 0.1908, 0.0291, 0.6928],\n",
      "         [0.2877, 0.8548, 0.7394, 0.1457, 0.2828, 0.0222, 0.9719, 0.8331,\n",
      "          0.3450, 0.0873, 0.4397, 0.0760],\n",
      "         [0.0965, 0.4549, 0.6765, 0.8307, 0.5361, 0.6237, 0.4570, 0.3490,\n",
      "          0.5878, 0.2956, 0.1151, 0.7451],\n",
      "         [0.2248, 0.9463, 0.5156, 0.1240, 0.5064, 0.6489, 0.7283, 0.2686,\n",
      "          0.7721, 0.3729, 0.1039, 0.8908],\n",
      "         [0.1624, 0.6752, 0.8542, 0.3142, 0.2070, 0.1302, 0.3621, 0.5767,\n",
      "          0.5807, 0.3117, 0.1114, 0.8456],\n",
      "         [0.2697, 0.1161, 0.3301, 0.5103, 0.1458, 0.4043, 0.5708, 0.8177,\n",
      "          0.8311, 0.9241, 0.7423, 0.7344],\n",
      "         [0.8604, 0.5618, 0.4336, 0.5197, 0.7332, 0.6024, 0.2580, 0.6312,\n",
      "          0.3810, 0.4059, 0.7945, 0.3454],\n",
      "         [0.6144, 0.6788, 0.2994, 0.3755, 0.8658, 0.1374, 0.4623, 0.9424,\n",
      "          0.6382, 0.6634, 0.3968, 0.1509],\n",
      "         [0.1467, 0.1969, 0.2479, 0.0276, 0.0958, 0.7527, 0.2241, 0.5509,\n",
      "          0.3202, 0.9239, 0.7862, 0.0402]]])\n"
     ]
    }
   ],
   "source": [
    "# 测试代码，正式demo中实际上不需要这个\n",
    "# import torch\n",
    "# audio_features = torch.rand((1, 10, 13))\n",
    "# print(audio_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given input size: (128x1x1). Calculated output size: (128x1x0). Output size is too small",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m model\u001b[39m.\u001b[39meval()  \u001b[39m# Change model to 'eval' mode (BN uses moving mean/var).\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m# result = model(audio_features_tmp, clip)\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m result \u001b[39m=\u001b[39m model(audio_features, clip)\n\u001b[1;32m     17\u001b[0m \u001b[39m# 下面是用来查看模型所在位置的测试代码（查看是在cpu上还是gpu上）\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39m# for param in model.parameters():\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39m#     print(param.device)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39m# print(audio_features.device)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39m# print(clip.device)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39mprint\u001b[39m(result\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/.conda/envs/multimodal-emotion-recognition/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/work_space/multimodal-emotion-recognition-experiment/models/multimodalcnn.py:251\u001b[0m, in \u001b[0;36mMultiModalCNN.forward\u001b[0;34m(self, x_audio, x_visual)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_feature_3(x_audio, x_visual)\n\u001b[1;32m    250\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfusion \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39miaLSTM\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 251\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_feature_4(x_audio, x_visual)\n",
      "File \u001b[0;32m~/work_space/multimodal-emotion-recognition-experiment/models/multimodalcnn.py:277\u001b[0m, in \u001b[0;36mMultiModalCNN.forward_feature_4\u001b[0;34m(self, x_audio, x_visual)\u001b[0m\n\u001b[1;32m    274\u001b[0m x_audio \u001b[39m=\u001b[39m h_va\u001b[39m*\u001b[39mx_audio\n\u001b[1;32m    275\u001b[0m x_visual \u001b[39m=\u001b[39m h_av\u001b[39m*\u001b[39mx_visual\n\u001b[0;32m--> 277\u001b[0m x_audio \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maudio_model\u001b[39m.\u001b[39;49mforward_stage2(x_audio)       \n\u001b[1;32m    278\u001b[0m x_visual \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvisual_model\u001b[39m.\u001b[39mforward_stage2(x_visual)\n\u001b[1;32m    280\u001b[0m audio_pooled \u001b[39m=\u001b[39m x_audio\u001b[39m.\u001b[39mmean([\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]) \u001b[39m#mean accross temporal dimension\u001b[39;00m\n",
      "File \u001b[0;32m~/work_space/multimodal-emotion-recognition-experiment/models/multimodalcnn.py:185\u001b[0m, in \u001b[0;36mAudioCNNPool.forward_stage2\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_stage2\u001b[39m(\u001b[39mself\u001b[39m,x):\n\u001b[1;32m    184\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1d_2(x)\n\u001b[0;32m--> 185\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1d_3(x)   \n\u001b[1;32m    186\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.conda/envs/multimodal-emotion-recognition/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/multimodal-emotion-recognition/lib/python3.9/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/multimodal-emotion-recognition/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/multimodal-emotion-recognition/lib/python3.9/site-packages/torch/nn/modules/pooling.py:88\u001b[0m, in \u001b[0;36mMaxPool1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> 88\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mmax_pool1d(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkernel_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m     89\u001b[0m                         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mceil_mode,\n\u001b[1;32m     90\u001b[0m                         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreturn_indices)\n",
      "File \u001b[0;32m~/.conda/envs/multimodal-emotion-recognition/lib/python3.9/site-packages/torch/_jit_internal.py:405\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    403\u001b[0m     \u001b[39mreturn\u001b[39;00m if_true(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    404\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 405\u001b[0m     \u001b[39mreturn\u001b[39;00m if_false(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/multimodal-emotion-recognition/lib/python3.9/site-packages/torch/nn/functional.py:652\u001b[0m, in \u001b[0;36m_max_pool1d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[39mif\u001b[39;00m stride \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    651\u001b[0m     stride \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mannotate(List[\u001b[39mint\u001b[39m], [])\n\u001b[0;32m--> 652\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mmax_pool1d(\u001b[39minput\u001b[39;49m, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given input size: (128x1x1). Calculated output size: (128x1x0). Output size is too small"
     ]
    }
   ],
   "source": [
    "# 模型输出\n",
    "testResource = ['/home/ubuntu/work_space/datasets/RAVDESS_autido_speech/Actor_20/02-01-03-01-02-02-20_facecroppad.npy',\n",
    "                '/home/ubuntu/work_space/datasets/RAVDESS_autido_speech/Actor_20/03-01-03-01-02-02-20_croppad.wav']\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 注意这里得赋值，不然张量不会转移到gpu上去\n",
    "audio_features = audio_features.to(device)\n",
    "clip = clip.to(device)\n",
    "model.to(device)\n",
    "model.cuda(1)   # 这里的0或者1代表你想使用哪块gpu\n",
    "\n",
    "# Test the Model\n",
    "model.eval()  # Change model to 'eval' mode (BN uses moving mean/var).\n",
    "\n",
    "# result = model(audio_features_tmp, clip)\n",
    "result = model(audio_features, clip)\n",
    "# 下面是用来查看模型所在位置的测试代码（查看是在cpu上还是gpu上）\n",
    "# for param in model.parameters():\n",
    "#     print(param.device)\n",
    "# print(audio_features.device)\n",
    "# print(clip.device)\n",
    "\n",
    "print(result.shape)\n",
    "print(result)\n",
    "print(sum(sum(result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 处理原视频，提取人脸位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "快乐\n"
     ]
    }
   ],
   "source": [
    "# 提取识别到的表情\n",
    "expression_list = [\"中性\", \"平静\", \"快乐\", \"悲伤\", \"愤怒\", \"恐惧\", \"厌恶\", \"惊讶\"]\n",
    "max_index = torch.argmax(result[0])\n",
    "expression = expression_list[max_index]\n",
    "print(expression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "from numpy.lib.function_base import _quantile_unchecked\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.backends import cudnn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "# from face_detection import RetinaFace\n",
    "# import retinaface as RetinaFace\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "# matplotlib.use('TkAgg')\n",
    "\n",
    "import utils\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"Parse input arguments.\"\"\"\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Head pose estimation using the 6DRepNet.')\n",
    "    parser.add_argument('--gpu',\n",
    "                        dest='gpu_id', help='GPU device id to use [0]',\n",
    "                        default=1, type=int)\n",
    "    # 尝试使用video作为应用源\n",
    "    parser.add_argument(\"--video\", type=str, default='/home/ubuntu/work_space/datasets/RAVDESS_autido_speech/Actor_03/01-01-03-02-01-01-03.mp4',\n",
    "                        help=\"Path of video to process i.e. /path/to/vid.mp4\")\n",
    "    parser.add_argument('--cam',\n",
    "                        dest='cam_id', help='Camera device id to use [0]',\n",
    "                        default=None, type=int) # 此处default在原文件中是0\n",
    "    parser.add_argument('--snapshot',\n",
    "                        dest='snapshot', help='Name of model snapshot.',\n",
    "                        default='', type=str)\n",
    "    parser.add_argument('--save_viz',\n",
    "                        dest='save_viz', help='Save images with pose cube.',\n",
    "                        default=False, type=bool)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "transformations = transforms.Compose([transforms.Resize(224),\n",
    "                                      transforms.CenterCrop(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "# args = parse_args()\n",
    "cudnn.enabled = True\n",
    "# gpu = args.gpu_id\n",
    "gpu = 1\n",
    "# cam = args.cam_id if args.cam_id is not None else args.video\n",
    "cam = '/home/ubuntu/work_space/datasets/RAVDESS_autido_speech/Actor_03/01-01-03-02-01-01-03.mp4'\n",
    "if(cam is None):\n",
    "    print('Camera or video not specified as argument, selecting default camera node (0) as input...')\n",
    "    cam = 0\n",
    "# snapshot_path = args.snapshot\n",
    "\n",
    "print('Loading data.')\n",
    "\n",
    "# detector = RetinaFace(gpu_id=gpu)\n",
    "\n",
    "cap = cv2.VideoCapture(cam)\n",
    "\n",
    "# 以下代码用于保存处理后的视频文件\n",
    "# 获取视频的FPS、宽度和高度\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# 定义视频编码器\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "\n",
    "# 创建输出视频的VideoWriter对象\n",
    "out = cv2.VideoWriter('../datasets/test/output_video_multimodal_expression.mp4', fourcc, fps, (width, height))\n",
    "\n",
    "# Check if the webcam is opened correctly\n",
    "if not cap.isOpened():\n",
    "    raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "frame_count = 0\n",
    "time_consume = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    frame_count += 1\n",
    "    if frame is None:\n",
    "        break\n",
    "\n",
    "    # faces = detector(frame)\n",
    "    bbox = mtcnn.detect(frame)\n",
    "\n",
    "    # for box, landmarks, score in faces:\n",
    "    if bbox[0] is not None:\n",
    "        bbox = bbox[0][0]\n",
    "        bbox = [round(x) for x in bbox]\n",
    "        x1, y1, x2, y2 = bbox\n",
    "\n",
    "        # Print the location of each face in this image\n",
    "        # if score < .95:\n",
    "        #     continue\n",
    "        # x_min = int(box[0])\n",
    "        # y_min = int(box[1])\n",
    "        # x_max = int(box[2])\n",
    "        # y_max = int(box[3])\n",
    "        x_min = x1\n",
    "        y_min = y1\n",
    "        x_max = x2\n",
    "        y_max = y2\n",
    "        bbox_width = abs(x_max - x_min)\n",
    "        bbox_height = abs(y_max - y_min)\n",
    "\n",
    "        x_min = max(0, x_min-int(0.2*bbox_height))\n",
    "        y_min = max(0, y_min-int(0.2*bbox_width))\n",
    "        x_max = x_max+int(0.2*bbox_height)\n",
    "        y_max = y_max+int(0.2*bbox_width)\n",
    "\n",
    "        # c = cv2.waitKey(1)\n",
    "        # if c == 27:\n",
    "        #     break\n",
    "\n",
    "        start = time.time()\n",
    "        Expression_pred = expression\n",
    "        end = time.time()\n",
    "        time_consume += (end - start)*1000.\n",
    "        # print('Head pose estimation: %2f ms' % ((end - start)*1000.))\n",
    "        # print(\"Processed frame per second: %2f fps\" % (1000. / (time_consume / frame_count)))\n",
    "        cv2.rectangle(frame, (int(x1),int(y1)), (int(x2),int(y2)), (0,255,0), 2)\n",
    "        #utils.draw_axis(frame, y_pred_deg, p_pred_deg, r_pred_deg, left+int(.5*(right-left)), top, size=100)\n",
    "        # 定义要添加的文字\n",
    "        expression = 'happy'\n",
    "        text = expression\n",
    "\n",
    "        # 定义文字的位置和字体\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        position = (x1, y1 - 20)\n",
    "        font_scale = 2\n",
    "        color = (0, 255, 0)\n",
    "        thickness = 2\n",
    "\n",
    "        # 在图像上添加文字\n",
    "        cv2.putText(frame, text, position, font, font_scale, color, thickness)\n",
    "\n",
    "    out.write(frame)\n",
    "    # cv2.imshow(\"Demo\", frame)\n",
    "    # cv2.waitKey(5)\n",
    "out.release()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 保存视频"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上面的代码已经保存了视频了\n",
    "# 我能否做出一个实时效果的展示视频呢？"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal-emotion-recognition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
