{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对长视频进行专注度量化并标注"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梳理思路\n",
    "\n",
    "梳理一下思路，目前打算将视频分成一个个小段来进行处理，每个小段需要依次进行以下操作：\n",
    "1. **提取帧序列**。<br>\n",
    "    对读取的这一小段视频按一定间隔进行逐帧提取画面，保存为一个帧序列。\n",
    "2. **进行人脸检测提取人脸序列**。<br>\n",
    "    对小段视频的第一帧进行人脸检测，之后以该帧上人脸的位置为基础提取之后的人脸帧序列。<br>\n",
    "    这样规避了人脸追踪的麻烦。<br>\n",
    "    *可以记录下第一帧的序号，之后处理视频时可能需要回到这一帧*。\n",
    "3. **将提取到的人脸序列转换为Dataloader需要的格式，送入模型**。<br>\n",
    "    需要将人脸序列进行格式转换，准备按序送入模型。\n",
    "4. **进行模型推理**。<br>\n",
    "    推理得到结果并存储到队列中，之后根据队列结果处理视频。\n",
    "5. **进行视频处理并保存**。<br>\n",
    "    视频转到第一帧保存的位置，按照处理结果对每一帧进行标记并保存结果。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 添加主要库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 添加依赖，主要是总依赖，包含EfficientFaceTemporal的初始化\n",
    "import time\n",
    "import math\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from numpy.lib.function_base import _quantile_unchecked\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.backends import cudnn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "# matplotlib.use('TkAgg')\n",
    "\n",
    "from models import multimodalcnn\n",
    "import utils\n",
    "\n",
    "from models.modulator import Modulator\n",
    "from models.efficientface import LocalFeatureExtractor, InvertedResidual\n",
    "from models.transformer_timm import AttentionBlock, Attention\n",
    "import torchvision.models as models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 人脸识别模型初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.conda/envs/multimodal-emotion-recognition/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.conda/envs/multimodal-emotion-recognition/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import numpy as np          \n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from facenet_pytorch import MTCNN\n",
    "from face_detection import RetinaFace\n",
    "\n",
    "cudnn.enabled = True\n",
    "gpu = 1\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "detector = RetinaFace(gpu_id=gpu, \\\n",
    "                      model_path = '/home/ubuntu/work_space/Pretrained_model/RetinaFace/Resnet50_Final.pth', \\\n",
    "                      network = \"resnet50\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 专注度识别模型相关内容初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先是模型前置的一些神经网络依赖\n",
    "def conv1d_block(in_channels, out_channels, kernel_size=3, stride=1, padding='same'):\n",
    "    return nn.Sequential(nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size,stride=stride, padding=padding),nn.BatchNorm1d(out_channels),\n",
    "                                   nn.ReLU(inplace=True)) \n",
    "\n",
    "class EfficientFaceTemporal(nn.Module):\n",
    "\n",
    "    def __init__(self, stages_repeats, stages_out_channels, num_classes=7, im_per_sample=25):\n",
    "        super(EfficientFaceTemporal, self).__init__()\n",
    "\n",
    "        if len(stages_repeats) != 3:\n",
    "            raise ValueError('expected stages_repeats as list of 3 positive ints')\n",
    "        if len(stages_out_channels) != 5:\n",
    "            raise ValueError('expected stages_out_channels as list of 5 positive ints')\n",
    "        self._stage_out_channels = stages_out_channels\n",
    "\n",
    "        input_channels = 3\n",
    "        output_channels = self._stage_out_channels[0]\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(input_channels, output_channels, 3, 2, 1, bias=False),\n",
    "                                   nn.BatchNorm2d(output_channels),\n",
    "                                   nn.ReLU(inplace=True),)\n",
    "        input_channels = output_channels\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        stage_names = ['stage{}'.format(i) for i in [2, 3, 4]]\n",
    "        for name, repeats, output_channels in zip(stage_names, stages_repeats, self._stage_out_channels[1:]):\n",
    "            seq = [InvertedResidual(input_channels, output_channels, 2)]\n",
    "            for i in range(repeats - 1):\n",
    "                seq.append(InvertedResidual(output_channels, output_channels, 1))\n",
    "            setattr(self, name, nn.Sequential(*seq))\n",
    "            input_channels = output_channels\n",
    "\n",
    "        self.local = LocalFeatureExtractor(29, 116, 1)\n",
    "        self.modulator = Modulator(116)\n",
    "\n",
    "        output_channels = self._stage_out_channels[-1]\n",
    "\n",
    "        self.conv5 = nn.Sequential(nn.Conv2d(input_channels, output_channels, 1, 1, 0, bias=False),\n",
    "                                   nn.BatchNorm2d(output_channels),\n",
    "                                   nn.ReLU(inplace=True),)\n",
    "        self.conv1d_0 = conv1d_block(output_channels, 64)\n",
    "        self.conv1d_1 = conv1d_block(64, 64)\n",
    "        self.conv1d_2 = conv1d_block(64, 128)\n",
    "        self.conv1d_3 = conv1d_block(128, 128)\n",
    "\n",
    "        # self.resnet50 = models.resnet50(pretrained=False)\n",
    "        self.resnet50 = models.resnet50(pretrained=True)\n",
    "        self.adaptive_avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "\n",
    "        self.classifier_1 = nn.Sequential(\n",
    "                nn.Linear(128, num_classes),\n",
    "            )\n",
    "        self.im_per_sample = im_per_sample\n",
    "        \n",
    "    def forward_features(self, x):  # torch.Size([1200, 3, 224, 224])\n",
    "        x = self.conv1(x)   # torch.Size([1200, 29, 112, 112])\n",
    "        x = self.maxpool(x) # torch.Size([1200, 29, 56, 56])\n",
    "        x = self.modulator(self.stage2(x)) + self.local(x)  # torch.Size([1200, 116, 28, 28])\n",
    "        x = self.stage3(x)  # torch.Size([1200, 232, 14, 14])\n",
    "        x = self.stage4(x)  # torch.Size([1200, 464, 7, 7])\n",
    "        x = self.conv5(x)   # torch.Size([1200, 1024, 7, 7])\n",
    "        # 对每个通道上的所有元素求平均值。这样就得到了一个一维向量作为输出\n",
    "        x = x.mean([2, 3]) #global average pooling， torch.Size([1200, 1024])\n",
    "        return x\n",
    "        \n",
    "    def forward_features_resnet(self, x):  # torch.Size([1200, 3, 224, 224])\n",
    "        x = self.resnet50.conv1(x)\n",
    "        x = self.resnet50.bn1(x)\n",
    "        x = self.resnet50.relu(x)\n",
    "        x = self.resnet50.maxpool(x)\n",
    "        x = self.resnet50.layer1(x)\n",
    "        x = self.resnet50.layer2(x)\n",
    "        x = self.resnet50.layer3(x)\n",
    "\n",
    "        x = self.adaptive_avgpool(x)\n",
    "        # x = torch.randn(1200, 1024, 7, 7)\n",
    "        # 对每个通道上的所有元素求平均值。这样就得到了一个一维向量作为输出\n",
    "        x = x.mean([2, 3]) #global average pooling， torch.Size([1200, 1024])\n",
    "        return x\n",
    "\n",
    "    def forward_stage1(self, x):\n",
    "        #Getting samples per batch\n",
    "        assert x.shape[0] % self.im_per_sample == 0, \"Batch size is not a multiple of sequence length.\"\n",
    "        n_samples = x.shape[0] // self.im_per_sample\n",
    "        x = x.view(n_samples, self.im_per_sample, x.shape[1])\n",
    "        x = x.permute(0,2,1)\n",
    "        x = self.conv1d_0(x)\n",
    "        x = self.conv1d_1(x)\n",
    "        return x\n",
    "        \n",
    "        \n",
    "    def forward_stage2(self, x):\n",
    "        x = self.conv1d_2(x)\n",
    "        x = self.conv1d_3(x)\n",
    "        return x\n",
    "    \n",
    "    def forward_classifier(self, x):\n",
    "        x = x.mean([-1]) #pooling accross temporal dimension\n",
    "        x1 = self.classifier_1(x)\n",
    "        return x1\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.forward_stage1(x)\n",
    "        x = self.forward_stage2(x)\n",
    "        x = self.forward_classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.conda/envs/multimodal-emotion-recognition/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing efficientnet\n",
      "<class 'models.multimodalcnn.MultiModalCNN'>\n"
     ]
    }
   ],
   "source": [
    "# 对模型的配置进行初始化\n",
    "num_classes = 4\n",
    "seq_length = 15\n",
    "num_heads = 1\n",
    "audio_input_chanel = 15\n",
    "pretrain_state_path = '/home/ubuntu/work_space/multimodal-emotion-recognition-experiment-engagement/best_results/1682136657.1589215lr_8.158608249130043e-05seed_42optimizer_Adamweight_decay_0.001_58.68/DAiSEE_multimodalcnn_15_best0.pth'\n",
    "pretrain_path = '/home/ubuntu/work_space/EfficientFace-master/checkpoint/Pretrained_EfficientFace.tar'\n",
    "model = multimodalcnn.MultiModalCNN(num_classes = num_classes, \\\n",
    "                                    fusion = 'iaLSTM', \\\n",
    "                                    seq_length = seq_length, \\\n",
    "                                    pretr_ef = pretrain_path, \\\n",
    "                                    num_heads = num_heads, \\\n",
    "                                    audio_input_chanel = audio_input_chanel)\n",
    "pretrained_state = torch.load(pretrain_state_path)\n",
    "pretrained_state_dict = pretrained_state['state_dict']\n",
    "# 这里要将一些字符串替换掉才能得到合适的字典\n",
    "pretrained_state_dict = {key.replace(\"module.\", \"\"): value for key, value in pretrained_state_dict.items()}\n",
    "model.load_state_dict(pretrained_state_dict)\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "model.cuda(1)   # 这里的0或者1代表你想使用哪块gpu\n",
    "\n",
    "# Test the Model\n",
    "model.eval()  # Change model to 'eval' mode (BN uses moving mean/var).\n",
    "\n",
    "print(type(model))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一些辅助函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_already(numpy_video_dataset, frames_per_interval):\n",
    "    for dataset in numpy_video_dataset:\n",
    "        if len(dataset) != frames_per_interval or len(dataset) == 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# 奖帧序列转换为dataloader的辅助函数\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, audio_feature_dataset, clips):\n",
    "        self.audio_feature_dataset = audio_feature_dataset\n",
    "        self.clips = clips\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.audio_feature_dataset), len(self.clips))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.audio_feature_dataset[idx], self.clips[idx]\n",
    "    \n",
    "import random\n",
    "# 返回最大的两个值所在的索引的其中一个，随机选择其中一个索引\n",
    "def rand_top2_index(result):\n",
    "    return (torch.topk(result, k=2).indices)[random.randint(0, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4)\n",
      "tensor([4, 3])\n",
      "tensor(3)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建一个张量\n",
    "x = torch.tensor([1, 3, 2, 4, 5])\n",
    "\n",
    "print(rand_top2_index(x))\n",
    "\n",
    "# 返回张量中前两个最大值的索引\n",
    "topk_indices = torch.topk(x, k=2).indices\n",
    "\n",
    "print(topk_indices)\n",
    "\n",
    "# 返回第二大的索引\n",
    "second_largest_index = topk_indices[1]\n",
    "\n",
    "# 打印结果\n",
    "print(second_largest_index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 头部姿态估计模型引入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SixDRepNet\n",
    "from sixdrepnet import SixDRepNet\n",
    "import cv2\n",
    "from math import cos, sin\n",
    "\n",
    "# Create model\n",
    "# Weights are automatically downloaded\n",
    "head_pose_model = SixDRepNet()\n",
    "\n",
    "# img = cv2.imread('/path/to/image.jpg')\n",
    "\n",
    "# pitch, yaw, roll = model.predict(img)\n",
    "\n",
    "# head_pose_model.draw_axis(img, yaw, pitch, roll)\n",
    "\n",
    "def get_head_pose_list(im, face_locations, head_pose_model):\n",
    "    head_pose_list = []\n",
    "    for i, [x1, y1, x2, y2] in enumerate(face_locations):\n",
    "        face_im = im[y1:y2, x1:x2, :]\n",
    "        face_im = cv2.resize(face_im, (224,224))\n",
    "        head_pose_list.append(head_pose_model.predict(face_im))\n",
    "    return head_pose_list\n",
    "\n",
    "def draw_axis(img, yaw, pitch, roll, tdx=None, tdy=None, size = 25):\n",
    "        \"\"\"\n",
    "        Prints the person's name and age.\n",
    "\n",
    "        If the argument 'additional' is passed, then it is appended after the main info.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        img : array\n",
    "            Target image to be drawn on\n",
    "        yaw : int\n",
    "            yaw rotation\n",
    "        pitch: int\n",
    "            pitch rotation\n",
    "        roll: int\n",
    "            roll rotation\n",
    "        tdx : int , optional\n",
    "            shift on x axis\n",
    "        tdy : int , optional\n",
    "            shift on y axis\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        img : array\n",
    "        \"\"\"\n",
    "\n",
    "        pitch = pitch * np.pi / 180\n",
    "        yaw = -(yaw * np.pi / 180)\n",
    "        roll = roll * np.pi / 180\n",
    "\n",
    "        if tdx != None and tdy != None:\n",
    "            tdx = tdx\n",
    "            tdy = tdy\n",
    "        else:\n",
    "            height, width = img.shape[:2]\n",
    "            tdx = width / 2\n",
    "            tdy = height / 2\n",
    "\n",
    "        # X-Axis pointing to right. drawn in red\n",
    "        x1 = size * (cos(yaw) * cos(roll)) + tdx\n",
    "        y1 = size * (cos(pitch) * sin(roll) + cos(roll) * sin(pitch) * sin(yaw)) + tdy\n",
    "\n",
    "        # Y-Axis | drawn in green\n",
    "        #        v\n",
    "        x2 = size * (-cos(yaw) * sin(roll)) + tdx\n",
    "        y2 = size * (cos(pitch) * cos(roll) - sin(pitch) * sin(yaw) * sin(roll)) + tdy\n",
    "\n",
    "        # Z-Axis (out of the screen) drawn in blue\n",
    "        x3 = size * (sin(yaw)) + tdx\n",
    "        y3 = size * (-cos(yaw) * sin(pitch)) + tdy\n",
    "\n",
    "        cv2.line(img, (int(tdx), int(tdy)), (int(x1),int(y1)),(0,0,255),4)\n",
    "        cv2.line(img, (int(tdx), int(tdy)), (int(x2),int(y2)),(0,255,0),4)\n",
    "        cv2.line(img, (int(tdx), int(tdy)), (int(x3),int(y3)),(255,0,0),4)\n",
    "\n",
    "        return img"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 图像处理相关函数初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "# 用于将帧序列转换为dataloader需要的格式\n",
    "import transforms\n",
    "video_transform = transforms.Compose([\n",
    "                    transforms.ToTensor(255)])\n",
    "\n",
    "# 这个函数每detect_faces_interval * fps帧运行一次，并以运行结果为标准进行人脸采集\n",
    "def get_face_location(im, detector):\n",
    "    face_location_list = []\n",
    "    faces = detector(im)\n",
    "    for box, _, score in faces:\n",
    "\n",
    "        # Print the location of each face in this image\n",
    "        if score < .20:\n",
    "            continue\n",
    "        x1 = int(box[0])\n",
    "        y1 = int(box[1])\n",
    "        \n",
    "        if 23 * x1 - 64 * y1 + 180 * 64 < 0:\n",
    "            continue\n",
    "\n",
    "        x2 = int(box[2])\n",
    "        y2 = int(box[3])\n",
    "\n",
    "        face_location_list.append([x1, y1, x2, y2])\n",
    "\n",
    "    # 下面这一句应该在我正式处理帧的时候使用，现在还不需要\n",
    "    # im = im[y1:y2, x1:x2, :]\n",
    "    # im = cv2.resize(im, (224,224))\n",
    "    return face_location_list\n",
    "\n",
    "# 对视频进行人脸的标注处理，这里并不属于最后的标注操作\n",
    "def mark_faces(im, faces):\n",
    "    for box in faces:\n",
    "        # Print the location of each face in this image\n",
    "        x_min = int(box[0])\n",
    "        y_min = int(box[1])\n",
    "        x_max = int(box[2])\n",
    "        y_max = int(box[3])\n",
    "        # 定义矩形的四个顶点坐标\n",
    "        x1, y1 = x_min, y_min\n",
    "        x2, y2 = x_max, y_max\n",
    "\n",
    "        # 在图像上绘制矩形\n",
    "        cv2.rectangle(im, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "    \n",
    "    return im\n",
    "\n",
    "# 对视频进行人脸的标注处理，这里并不属于最后的标注操作\n",
    "def mark_faces_by_index(im, faces, max_indexs):\n",
    "    for i, box in enumerate(faces):\n",
    "        # Print the location of each face in this image\n",
    "        x_min = int(box[0])\n",
    "        y_min = int(box[1])\n",
    "        x_max = int(box[2])\n",
    "        y_max = int(box[3])\n",
    "        # 定义矩形的四个顶点坐标\n",
    "        x1, y1 = x_min, y_min\n",
    "        x2, y2 = x_max, y_max\n",
    "\n",
    "        # 通过index确定人脸框的颜色\n",
    "        colors = [(0, 0, 255), (0, 255, 255), (0, 255, 0), (0, 255, 0)]\n",
    "\n",
    "        # 在图像上绘制矩形\n",
    "        cv2.rectangle(im, (x1, y1), (x2, y2), colors[max_indexs[i]], 2)\n",
    "    \n",
    "    return im\n",
    "\n",
    "# 对视频进行人脸的标注处理，这里并不属于最后的标注操作\n",
    "def mark_faces_by_index_with_text(im, faces, max_indexs, text=None):\n",
    "    for i, box in enumerate(faces):\n",
    "        # Print the location of each face in this image\n",
    "        x_min = int(box[0])\n",
    "        y_min = int(box[1])\n",
    "        x_max = int(box[2])\n",
    "        y_max = int(box[3])\n",
    "        # 定义矩形的四个顶点坐标\n",
    "        x1, y1 = x_min, y_min\n",
    "        x2, y2 = x_max, y_max\n",
    "\n",
    "        # 通过index确定人脸框的颜色\n",
    "        colors = [(0, 0, 255), (0, 255, 255), (0, 255, 0), (0, 255, 0)]\n",
    "        color = colors[max_indexs[i]]\n",
    "\n",
    "        # 设置文本参数\n",
    "        text = str(i)\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        font_scale = 0.5\n",
    "        thickness = 1\n",
    "        line_type = cv2.LINE_AA\n",
    "\n",
    "        # 在图像上添加文本\n",
    "        cv2.putText(im, text, (x_min, y_min), font, font_scale, color, thickness, line_type)\n",
    "\n",
    "        # 在图像上绘制矩形\n",
    "        cv2.rectangle(im, (x1, y1), (x2, y2), color, 2)\n",
    "    \n",
    "    return im\n",
    "\n",
    "# 对视频进行人脸和头部姿态的标注处理，这里并不属于最后的标注操作\n",
    "def mark_faces_by_index_with_text_and_headpose(im, faces, max_indexs, head_pose_list, text=None):\n",
    "    for i, box in enumerate(faces):\n",
    "        # Print the location of each face in this image\n",
    "        x_min = int(box[0])\n",
    "        y_min = int(box[1])\n",
    "        x_max = int(box[2])\n",
    "        y_max = int(box[3])\n",
    "        # 定义矩形的四个顶点坐标\n",
    "        x1, y1 = x_min, y_min\n",
    "        x2, y2 = x_max, y_max\n",
    "\n",
    "        # 通过index确定人脸框的颜色\n",
    "        colors = [(0, 0, 255), (0, 255, 255), (0, 255, 0), (0, 255, 0)]\n",
    "        color = colors[max_indexs[i]]\n",
    "\n",
    "        # 设置文本参数\n",
    "        text = str(i)\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        font_scale = 0.5\n",
    "        thickness = 1\n",
    "        line_type = cv2.LINE_AA\n",
    "\n",
    "        pitch, yaw, roll = head_pose_list[i]\n",
    "\n",
    "        draw_axis(im, yaw, pitch, roll, (x_min + x_max) / 2, (y_min + y_max) / 2,)\n",
    "\n",
    "        # 在图像上添加文本\n",
    "        cv2.putText(im, text, (x_min, y_min), font, font_scale, color, thickness, line_type)\n",
    "\n",
    "        # 在图像上绘制矩形\n",
    "        cv2.rectangle(im, (x1, y1), (x2, y2), color, 2)\n",
    "    \n",
    "    return im\n",
    "\n",
    "# 将提取出来的帧序列转换为dataloader需要的格式\n",
    "def frames_to_dataset(numpy_video_dataset):\n",
    "    # print(type(numpy_video_dataset))\n",
    "    lengths = [len(x) for x in numpy_video_dataset]\n",
    "    # print(lengths)\n",
    "    # print(numpy_video_dataset[0].shape)\n",
    "    # print(numpy_video_dataset)\n",
    "\n",
    "    self_spatial_transform = video_transform\n",
    "    self_spatial_transform.randomize_parameters()\n",
    "    clips = numpy_video_dataset\n",
    "    clips = [[self_spatial_transform(img) for img in clip] for clip in clips]\n",
    "    clips = [torch.stack(clip, 0) for clip in clips]\n",
    "    # print(type(clips))\n",
    "    # print([clip.shape for clip in clips])\n",
    "\n",
    "    return clips\n",
    "\n",
    "\n",
    "# 第一个参数是原始的音频序列，第二个参数是帧序列转换来的数据\n",
    "def get_dataloader(audio_feature_dataset, clips):\n",
    "    if audio_feature_dataset is None:\n",
    "        # 需要生成一个合适的数据格式，这里直接生成空的\n",
    "        audio_feature_dataset = np.zeros((len(clips), 1, 15, 18))\n",
    "    # 注意这里得赋值，不然张量不会转移到gpu上去\n",
    "    my_dataset = MyDataset(audio_feature_dataset, clips)\n",
    "    my_dataloader = DataLoader(my_dataset, \n",
    "                            batch_size=1, \n",
    "                            pin_memory=True)\n",
    "    # my_dataloader = my_dataloader.to(device)\n",
    "    return my_dataloader\n",
    "\n",
    "# 模型输出\n",
    "def get_model_output(model, my_dataloader):\n",
    "    # testResource = ['/home/ubuntu/work_space/datasets/RAVDESS_autido_speech/Actor_20/02-01-03-01-02-02-20_facecroppad.npy',\n",
    "                    # '/home/ubuntu/work_space/datasets/RAVDESS_autido_speech/Actor_20/03-01-03-01-02-02-20_croppad.wav']\n",
    "    # result = model(audio_features_tmp, clip)\n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        for i, (audio_features, clip) in enumerate(my_dataloader):\n",
    "            audio_features = audio_features.float()\n",
    "            audio_features = audio_features.to(device)\n",
    "            clip = clip.to(device)\n",
    "            audio_features = Variable(audio_features)\n",
    "            clip = Variable(clip)\n",
    "            audio_features = audio_features[0]\n",
    "            clip = clip[0]\n",
    "            temp_results = model(audio_features, clip)\n",
    "            results.append(temp_results)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 15, 18)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "npy_path = '/home/ubuntu/work_space/datasets/DAiSEE/DataSet/Validation/591292/5912920223/5912920223_facecroppad_facecroppad_headpose.npy'\n",
    "test = np.load(npy_path)\n",
    "\n",
    "print(test.shape)\n",
    "print(type(test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 主要参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_frames = 15\n",
    "input_fps = 30\n",
    "\n",
    "save_length = 3.6 # seconds\n",
    "save_avi = True # False\n",
    "\n",
    "failed_videos = []\n",
    "root = '/home/ubuntu/Videos/教学视频/example/'\n",
    "\n",
    "# 这段代码定义了一个lambda函数select_distributed，\n",
    "# 它的作用是将视频的帧数分成若干段，然后在每一段中均匀地选择一些帧。\n",
    "# 具体来说，它接受两个参数m和n，其中m表示要分成的段数，n表示视频的总帧数。\n",
    "# 它返回一个长度为m的列表，列表中的每个元素表示在对应的段中选择的帧的索引。\n",
    "# 这个函数在后面的代码中被用来选择视频中的一些帧进行人脸检测和裁剪。\n",
    "select_distributed = lambda m, n: [i*n//m + n//(2*m) for i in range(m)]\n",
    "n_processed = 0\n",
    "\n",
    "file_path = \"A2021-20222021-2022-2+北京航空航天大学++教学班+大学计算机基础+孙青_学生_03.01_9.50_example.mp4\"  # test_expressions.mp4\n",
    "# file_path = \"B2021-20222021-2022-2+北京航空航天大学++教学班+大学计算机基础+孙青_学生_03.01_14.00_example.mp4\"  # test_expressions.mp4\n",
    "filename = os.path.join(root, file_path)\n",
    "\n",
    "common_interval = 3\n",
    "detect_faces_interval = common_interval  # 时间间隔，单位为秒\n",
    "time_interval = common_interval  # 时间间隔，单位为秒\n",
    "\n",
    "engagement_list_English = [\"very low\", \"low\", \"high\", \"very high\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 主要流程开始"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 以下为伪代码\n",
    "if file_name is mp4:\n",
    "    读取视频\n",
    "    人脸检测出第一帧的所有人脸位置\n",
    "    将第一帧中的人脸图片存储到一个列表中\n",
    "        列表的每个元素均为一个列表，人脸图片存储在内列表中，为第一个元素中\n",
    "    处理人脸序列的数据并装载到模型中\n",
    "    模型推理得到结果，为一个数字列表    # 此处可以直接统计某一时段内的专注度情况\n",
    "    按照数字列表内容对所有人脸进行标准\n",
    "    将视频重新设置到第一帧的位置并按照模型推理结果进行标注\n",
    "    保存视频"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fps 25.0\n",
      "total_frames 3000\n",
      "video_duration: 120.0\n",
      "frames_interval: 5\n",
      "0 (array([2.7122226], dtype=float32), array([13.563397], dtype=float32), array([-1.2326567], dtype=float32))\n",
      "1 (array([2.1447291], dtype=float32), array([20.023413], dtype=float32), array([0.3993183], dtype=float32))\n",
      "2 (array([-1.7745497], dtype=float32), array([39.57921], dtype=float32), array([-8.449322], dtype=float32))\n",
      "3 (array([12.011671], dtype=float32), array([-70.632034], dtype=float32), array([-9.016001], dtype=float32))\n",
      "4 (array([8.354824], dtype=float32), array([-10.258587], dtype=float32), array([-28.82861], dtype=float32))\n",
      "5 (array([-3.8902783], dtype=float32), array([9.133473], dtype=float32), array([13.532314], dtype=float32))\n",
      "6 (array([7.678337], dtype=float32), array([-19.989618], dtype=float32), array([-11.1303], dtype=float32))\n",
      "7 (array([27.888577], dtype=float32), array([-46.614445], dtype=float32), array([-22.606546], dtype=float32))\n",
      "8 (array([-36.375473], dtype=float32), array([61.27941], dtype=float32), array([-33.525696], dtype=float32))\n",
      "9 (array([0.31334773], dtype=float32), array([18.620062], dtype=float32), array([-13.707408], dtype=float32))\n",
      "10 (array([-1.1751761], dtype=float32), array([52.718845], dtype=float32), array([-2.1228359], dtype=float32))\n",
      "11 (array([-8.643946], dtype=float32), array([-53.225056], dtype=float32), array([8.197186], dtype=float32))\n",
      "12 (array([-16.311377], dtype=float32), array([61.239433], dtype=float32), array([-14.832089], dtype=float32))\n",
      "13 (array([-13.223156], dtype=float32), array([35.288715], dtype=float32), array([-12.444993], dtype=float32))\n",
      "14 (array([-15.789897], dtype=float32), array([64.849014], dtype=float32), array([-12.610816], dtype=float32))\n",
      "15 (array([-17.899122], dtype=float32), array([55.275032], dtype=float32), array([-18.489464], dtype=float32))\n",
      "16 (array([-9.994946], dtype=float32), array([43.081062], dtype=float32), array([-15.489868], dtype=float32))\n",
      "17 (array([12.456476], dtype=float32), array([-52.445297], dtype=float32), array([-7.627318], dtype=float32))\n",
      "18 (array([26.372559], dtype=float32), array([-72.75566], dtype=float32), array([-14.865106], dtype=float32))\n",
      "19 (array([33.7192], dtype=float32), array([-46.35682], dtype=float32), array([-32.820423], dtype=float32))\n",
      "20 (array([10.049079], dtype=float32), array([-34.007046], dtype=float32), array([-14.570314], dtype=float32))\n",
      "21 (array([-15.682165], dtype=float32), array([-7.713143], dtype=float32), array([-8.981501], dtype=float32))\n",
      "22 (array([-38.306328], dtype=float32), array([72.92762], dtype=float32), array([-30.569662], dtype=float32))\n",
      "23 (array([8.412938], dtype=float32), array([-64.06199], dtype=float32), array([-3.9823165], dtype=float32))\n",
      "24 (array([-52.353596], dtype=float32), array([70.91061], dtype=float32), array([-49.42245], dtype=float32))\n",
      "25 (array([31.255182], dtype=float32), array([-72.74524], dtype=float32), array([-19.341484], dtype=float32))\n",
      "26 (array([-2.7337604], dtype=float32), array([16.858059], dtype=float32), array([-4.2080383], dtype=float32))\n",
      "27 (array([25.379732], dtype=float32), array([-51.241596], dtype=float32), array([-25.941492], dtype=float32))\n",
      "28 (array([18.53002], dtype=float32), array([-49.205147], dtype=float32), array([-28.010014], dtype=float32))\n",
      "29 (array([35.283092], dtype=float32), array([-72.823364], dtype=float32), array([-30.10751], dtype=float32))\n",
      "30 (array([-5.8622212], dtype=float32), array([51.16236], dtype=float32), array([1.0963463], dtype=float32))\n",
      "31 (array([-20.645103], dtype=float32), array([71.78284], dtype=float32), array([-17.173708], dtype=float32))\n",
      "32 (array([-9.798032], dtype=float32), array([-9.203244], dtype=float32), array([7.181734], dtype=float32))\n",
      "33 (array([-5.444491], dtype=float32), array([61.272682], dtype=float32), array([0.3602934], dtype=float32))\n",
      "34 (array([52.450775], dtype=float32), array([-54.01219], dtype=float32), array([-51.875595], dtype=float32))\n",
      "35 (array([2.1815066], dtype=float32), array([-53.284298], dtype=float32), array([9.526941], dtype=float32))\n",
      "36 (array([15.077926], dtype=float32), array([-62.341473], dtype=float32), array([-5.3510137], dtype=float32))\n",
      "37 (array([-7.4069023], dtype=float32), array([28.483961], dtype=float32), array([-17.21793], dtype=float32))\n",
      "38 (array([30.172049], dtype=float32), array([-76.029076], dtype=float32), array([-14.179942], dtype=float32))\n",
      "[tensor(3, device='cuda:1'), tensor(3, device='cuda:1'), tensor(3, device='cuda:1'), tensor(2, device='cuda:1'), tensor(2, device='cuda:1'), tensor(2, device='cuda:1'), tensor(3, device='cuda:1'), tensor(2, device='cuda:1'), tensor(1, device='cuda:1'), tensor(3, device='cuda:1'), tensor(1, device='cuda:1'), tensor(3, device='cuda:1'), tensor(3, device='cuda:1'), tensor(2, device='cuda:1'), tensor(3, device='cuda:1'), tensor(2, device='cuda:1'), tensor(2, device='cuda:1'), tensor(2, device='cuda:1'), tensor(2, device='cuda:1'), tensor(3, device='cuda:1'), tensor(2, device='cuda:1'), tensor(3, device='cuda:1'), tensor(2, device='cuda:1'), tensor(2, device='cuda:1'), tensor(2, device='cuda:1'), tensor(3, device='cuda:1'), tensor(2, device='cuda:1'), tensor(2, device='cuda:1'), tensor(2, device='cuda:1'), tensor(2, device='cuda:1'), tensor(3, device='cuda:1'), tensor(2, device='cuda:1'), tensor(3, device='cuda:1'), tensor(2, device='cuda:1'), tensor(2, device='cuda:1'), tensor(2, device='cuda:1'), tensor(3, device='cuda:1'), tensor(2, device='cuda:1'), tensor(2, device='cuda:1')]\n",
      "[tensor(3, device='cuda:1'), tensor(2, device='cuda:1'), tensor(2, device='cuda:1'), tensor(3, device='cuda:1'), tensor(3, device='cuda:1'), tensor(2, device='cuda:1'), tensor(3, device='cuda:1'), tensor(2, device='cuda:1'), tensor(2, device='cuda:1'), tensor(2, device='cuda:1'), tensor(2, device='cuda:1'), tensor(2, device='cuda:1'), tensor(3, device='cuda:1'), tensor(3, device='cuda:1'), tensor(2, device='cuda:1'), tensor(3, device='cuda:1'), tensor(3, device='cuda:1'), tensor(2, device='cuda:1'), tensor(3, device='cuda:1'), tensor(2, device='cuda:1'), tensor(3, device='cuda:1'), tensor(2, device='cuda:1'), tensor(2, device='cuda:1'), tensor(2, device='cuda:1'), tensor(3, device='cuda:1'), tensor(3, device='cuda:1'), tensor(1, device='cuda:1'), tensor(2, device='cuda:1'), tensor(2, device='cuda:1'), tensor(2, device='cuda:1'), tensor(2, device='cuda:1'), tensor(3, device='cuda:1'), tensor(3, device='cuda:1'), tensor(2, device='cuda:1'), tensor(3, device='cuda:1'), tensor(2, device='cuda:1'), tensor(2, device='cuda:1'), tensor(3, device='cuda:1'), tensor(2, device='cuda:1')]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# 主要处理流程\n",
    "# 以下代码针对目标视频文件进行专注度识别，其中只用到了\n",
    "if filename.endswith('.mp4'):\n",
    "\n",
    "    mtcnn = []\n",
    "                \n",
    "    cap = cv2.VideoCapture(filename)\n",
    "    #calculate length in frames\n",
    "    # 获取视频帧率和总帧数，教学视频的帧率好像是25，有点特殊，需要注意\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # 计算视频时长\n",
    "    video_duration = total_frames / fps\n",
    "    print('fps', fps)   # 帧率\n",
    "    print('total_frames', total_frames) # 总帧数\n",
    "    print('video_duration:', video_duration)    # 视频总长度\n",
    "\n",
    "    # 定义每个时间间隔内的帧数\n",
    "    frames_per_interval = 15  # 每个时间间隔内抽取15帧\n",
    "\n",
    "    # 抽取的帧之间的间隔帧数\n",
    "    frames_interval = int((time_interval * math.ceil(fps)) / frames_per_interval)\n",
    "    print('frames_interval:', frames_interval)\n",
    "\n",
    "    # save_fps = int(frames_per_interval / time_interval)\n",
    "    save_fps = fps\n",
    "\n",
    "    face_locations = []\n",
    "    max_indexs = []\n",
    "    numpy_video = []\n",
    "    numpy_video_dataset = []\n",
    "    success = 0\n",
    "    frame_cnt = 0\n",
    "\n",
    "    interval_begin_frame = 0\n",
    "\n",
    "    head_pose_list = []\n",
    "\n",
    "    # 可以从第一帧开始每一帧都判断是否需要采集，需要就放到数组里面\n",
    "    # 判断采集满了就放到总数据集里面\n",
    "    # out = cv2.VideoWriter(filename[:-4]+'_face_detect' + '.avi', \\\n",
    "    #                        cv2.VideoWriter_fourcc('M','J','P','G'), \\\n",
    "    #                        save_fps, (1280,720))\n",
    "    out = cv2.VideoWriter(filename[:-4]+'_face_detect' + '.mp4', \\\n",
    "                      cv2.VideoWriter_fourcc(*'mp4v'), \\\n",
    "                      save_fps, (1280,720))\n",
    "    while frame_cnt < total_frames:\n",
    "        ret, im = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # 如果不是需要采样的帧，则跳过\n",
    "        # 若开启保存选项，则此处跳过的帧会在推理完成后回退处理并记录到保存的文件中\n",
    "        if frame_cnt % frames_interval != 0:\n",
    "            frame_cnt += 1\n",
    "            continue\n",
    "\n",
    "        if frame_cnt % (detect_faces_interval * math.ceil(fps)) == 0:\n",
    "            interval_begin_frame = frame_cnt\n",
    "            face_locations = get_face_location(im, detector)\n",
    "            numpy_video_dataset = [[] for _ in range(len(face_locations))]\n",
    "            head_pose_list = get_head_pose_list(im, face_locations, head_pose_model)\n",
    "            for i, head_pose in enumerate(head_pose_list):\n",
    "                print(i, head_pose)\n",
    "\n",
    "        for i, [x1, y1, x2, y2] in enumerate(face_locations):\n",
    "            face_im = im[y1:y2, x1:x2, :]\n",
    "            face_im = cv2.resize(face_im, (224,224))\n",
    "            if len(face_locations) != len(numpy_video_dataset):\n",
    "                print('frame_cnt:', frame_cnt)\n",
    "                print(len(face_locations))\n",
    "                print(len(numpy_video_dataset))\n",
    "            numpy_video_dataset[i].append(face_im)\n",
    "\n",
    "        frame_cnt += 1\n",
    "\n",
    "        if frame_cnt % 100 == 0:\n",
    "            print('cur_frame:', frame_cnt)\n",
    "\n",
    "        if check_data_already(numpy_video_dataset, frames_per_interval):\n",
    "            current_frame = frame_cnt\n",
    "\n",
    "            # 处理数据将其准备成可以输入模型的格式\n",
    "            my_dataset = frames_to_dataset(numpy_video_dataset)\n",
    "            my_dataloader = get_dataloader(None, my_dataset)\n",
    "\n",
    "            # 输入模型得到结果\n",
    "            results = get_model_output(model, my_dataloader)\n",
    "            # print([len(result[0]) for result in results])\n",
    "            # print(results)\n",
    "\n",
    "            # 统计这一段时间内的专注度分布情况\n",
    "            engagement_list_English = [\"very low\", \"low\", \"high\", \"very high\"]\n",
    "            max_indexs = [rand_top2_index(result[0]) for result in results]\n",
    "            # max_indexs = [torch.argmax(result[0]) for result in results]\n",
    "            # engagements = [engagement_list_English[max_index] for max_index in max_indexs]\n",
    "            engagements = max_indexs\n",
    "            print(engagements)\n",
    "            # print(len(engagements))\n",
    "\n",
    "            # 如果需要处理视频的话在此处进行处理\n",
    "            if len(face_locations) == len(max_indexs) and save_avi:\n",
    "                # 设置视频的帧数\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, interval_begin_frame)\n",
    "                # for i in range(interval_begin_frame, frame_cnt):\n",
    "                for i in range(detect_faces_interval * math.ceil(fps)):\n",
    "                    ret, im = cap.read()\n",
    "                    im = mark_faces_by_index_with_text_and_headpose(im, \\\n",
    "                                                                    face_locations, \\\n",
    "                                                                    max_indexs, \\\n",
    "                                                                    head_pose_list=head_pose_list)\n",
    "                    out.write(im)\n",
    "\n",
    "            # clear the data\n",
    "            numpy_video_dataset = []\n",
    "\n",
    "    if len(numpy_video_dataset) > 0 and not check_data_already(numpy_video_dataset, frames_per_interval):\n",
    "        for numpy_video in numpy_video_dataset:\n",
    "            while len(numpy_video) < frames_per_interval:\n",
    "                numpy_video.append(np.zeros((224,224,3), dtype=np.uint8))\n",
    "        # if save_avi:\n",
    "        #     out.release()\n",
    "\n",
    "    if check_data_already(numpy_video_dataset, frames_per_interval):\n",
    "        # 处理数据将其准备成可以输入模型的格式\n",
    "        my_dataset = frames_to_dataset(numpy_video_dataset)\n",
    "        my_dataloader = get_dataloader(None, my_dataset)\n",
    "\n",
    "        # 输入模型得到结果\n",
    "        results = get_model_output(model, my_dataloader)\n",
    "\n",
    "        # 统计这一段时间内的专注度分布情况\n",
    "        engagement_list_English = [\"very low\", \"low\", \"high\", \"very high\"]\n",
    "        max_indexs = [rand_top2_index(result[0]) for result in results]\n",
    "        # max_indexs = [torch.argmax(result[0]) for result in results]\n",
    "        # engagements = [engagement_list_English[max_index] for max_index in max_indexs]\n",
    "        engagements = max_indexs\n",
    "        print(engagements)\n",
    "\n",
    "        # 如果需要处理视频的话在此处进行处理\n",
    "        if len(face_locations) == len(max_indexs) and save_avi:\n",
    "            # 设置视频的帧数\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, interval_begin_frame)\n",
    "            # for i in range(interval_begin_frame, frame_cnt):\n",
    "            for i in range(detect_faces_interval * math.ceil(fps)):\n",
    "                ret, im = cap.read()\n",
    "                im = mark_faces_by_index(im, face_locations, max_indexs)\n",
    "                out.write(im)\n",
    "\n",
    "        # clear the data\n",
    "        numpy_video_dataset = []\n",
    "    if save_avi and out.isOpened():\n",
    "        out.release()\n",
    "\n",
    "    # np.save(filename[:-4]+'_facecroppad.npy', np.array(numpy_video))\n",
    "    # numpy_video_dataset = np.array(numpy_video_dataset)\n",
    "    print(len(numpy_video_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[]\n",
      "<class 'list'>\n",
      "0\n",
      "<class 'list'>\n",
      "[]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 处理视频并记录数据"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 需要用到的辅助函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "3.52\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "# 返回最大的两个值的加权和\n",
    "# 四舍五入在通过头部姿态估计信息调整后进行\n",
    "def top2_index_weighted_sum(result, weight1=0.7, weight2=0.3):\n",
    "    top2_indeces = torch.topk(result, k=2).indices\n",
    "    return top2_indeces[0] * weight1 + top2_indeces[1] * weight2\n",
    "\n",
    "print(round(3.5)) # 4\n",
    "print(round(3.523, 2)) # 3.52\n",
    "\n",
    "# 通过得到的头部姿态信息调整专注度量化值\n",
    "def adjust_indexs_by_head_pose(max_indexs, head_pose_list):\n",
    "    new_indexs = []\n",
    "    for i, index in enumerate(max_indexs):\n",
    "        for pose in head_pose_list[i]:\n",
    "            if pose < -45:\n",
    "                index -= 0.4\n",
    "            # break\n",
    "        index = torch.round(index)\n",
    "        new_indexs.append(index)\n",
    "    return new_indexs\n",
    "\n",
    "def check_all_beyond(list, bounder):\n",
    "    for item in list:\n",
    "        if item < bounder:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# 以下是一些用于统计计算的辅助函数\n",
    "# 统计满足某种条件的头部姿态人数\n",
    "def get_pose_low(head_pose_list):\n",
    "    pitch_low = len([pose for pose in head_pose_list if pose[0] < -45])\n",
    "    yaw_low = len([pose for pose in head_pose_list if pose[1] < -45])\n",
    "    roll_low = len([pose for pose in head_pose_list if pose[2] < -45])\n",
    "    head_up = len([pose for pose in head_pose_list if check_all_beyond(pose, -45)])\n",
    "    return pitch_low, yaw_low, roll_low, head_up\n",
    "\n",
    "# 统计各类专注度的数量\n",
    "def get_engage(max_indexs):\n",
    "    engage_0 = len([index for index in max_indexs if index == 0])\n",
    "    engage_1 = len([index for index in max_indexs if index == 1])\n",
    "    engage_2 = len([index for index in max_indexs if index == 2])\n",
    "    engage_3 = len([index for index in max_indexs if index == 3])\n",
    "    return engage_0, engage_1, engage_2, engage_3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理单条视频"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fps 25.0\n",
      "total_frames 3000\n",
      "video_duration: 120.0\n",
      "frames_interval: 5\n",
      "03.01_9.50_example.mp4 0.0 finished\n",
      "03.01_9.50_example.mp4 3.0 finished\n",
      "03.01_9.50_example.mp4 6.0 finished\n",
      "03.01_9.50_example.mp4 9.0 finished\n",
      "03.01_9.50_example.mp4 12.0 finished\n",
      "03.01_9.50_example.mp4 15.0 finished\n",
      "03.01_9.50_example.mp4 18.0 finished\n",
      "03.01_9.50_example.mp4 21.0 finished\n",
      "03.01_9.50_example.mp4 24.0 finished\n",
      "03.01_9.50_example.mp4 27.0 finished\n",
      "03.01_9.50_example.mp4 30.0 finished\n",
      "03.01_9.50_example.mp4 33.0 finished\n",
      "03.01_9.50_example.mp4 36.0 finished\n",
      "03.01_9.50_example.mp4 39.0 finished\n",
      "03.01_9.50_example.mp4 42.0 finished\n",
      "03.01_9.50_example.mp4 45.0 finished\n",
      "03.01_9.50_example.mp4 48.0 finished\n",
      "03.01_9.50_example.mp4 51.0 finished\n",
      "03.01_9.50_example.mp4 54.0 finished\n",
      "03.01_9.50_example.mp4 57.0 finished\n",
      "03.01_9.50_example.mp4 60.0 finished\n",
      "03.01_9.50_example.mp4 63.0 finished\n",
      "03.01_9.50_example.mp4 66.0 finished\n",
      "03.01_9.50_example.mp4 69.0 finished\n",
      "03.01_9.50_example.mp4 72.0 finished\n",
      "03.01_9.50_example.mp4 75.0 finished\n",
      "03.01_9.50_example.mp4 78.0 finished\n",
      "03.01_9.50_example.mp4 81.0 finished\n",
      "03.01_9.50_example.mp4 84.0 finished\n",
      "03.01_9.50_example.mp4 87.0 finished\n",
      "03.01_9.50_example.mp4 90.0 finished\n",
      "03.01_9.50_example.mp4 93.0 finished\n",
      "03.01_9.50_example.mp4 96.0 finished\n",
      "03.01_9.50_example.mp4 99.0 finished\n",
      "03.01_9.50_example.mp4 102.0 finished\n",
      "03.01_9.50_example.mp4 105.0 finished\n",
      "03.01_9.50_example.mp4 108.0 finished\n",
      "03.01_9.50_example.mp4 111.0 finished\n",
      "03.01_9.50_example.mp4 114.0 finished\n",
      "03.01_9.50_example.mp4 117.0 finished\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# 处理单条视频尝试得到想要的csv文件\n",
    "parent_path = '/home/ubuntu/Videos/教学视频/example'\n",
    "file_path = 'A2021-20222021-2022-2+北京航空航天大学++教学班+大学计算机基础+孙青_学生_03.01_9.50_example.mp4'\n",
    "filename = os.path.join(parent_path, file_path)\n",
    "\n",
    "if filename.endswith('.mp4'):\n",
    "\n",
    "    cap = cv2.VideoCapture(filename)\n",
    "    #calculate length in frames\n",
    "    # 获取视频帧率和总帧数，教学视频的帧率好像是25，有点特殊，需要注意\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # 计算视频时长\n",
    "    video_duration = total_frames / fps\n",
    "    print('fps', fps)   # 帧率\n",
    "    print('total_frames', total_frames) # 总帧数\n",
    "    print('video_duration:', video_duration)    # 视频总长度\n",
    "\n",
    "    # 定义每个时间间隔内的帧数\n",
    "    frames_per_interval = 15  # 每个时间间隔内抽取15帧\n",
    "\n",
    "    # 抽取的帧之间的间隔帧数\n",
    "    frames_interval = int((time_interval * math.ceil(fps)) / frames_per_interval)\n",
    "    print('frames_interval:', frames_interval)\n",
    "\n",
    "    # save_fps = int(frames_per_interval / time_interval)\n",
    "    save_fps = fps\n",
    "    # 这里不需要保存视频，只需要对视频进行处理即可\n",
    "    save_avi = False\n",
    "\n",
    "    face_locations = []\n",
    "    max_indexs = []\n",
    "    numpy_video = []\n",
    "    numpy_video_dataset = []\n",
    "    success = 0\n",
    "    frame_cnt = 0\n",
    "\n",
    "    interval_begin_frame = 0\n",
    "    head_pose_list = []\n",
    "\n",
    "    # 可以从第一帧开始每一帧都判断是否需要采集，需要就放到数组里面\n",
    "    # 判断采集满了就放到总数据集里面\n",
    "    out = cv2.VideoWriter(filename[:-4]+'_face_detect' + '.mp4', \\\n",
    "                          cv2.VideoWriter_fourcc(*'mp4v'), \\\n",
    "                          save_fps, \\\n",
    "                          (1280,720))\n",
    "    \n",
    "    header = ['begin_stamp', \n",
    "              'pitch_low', 'yaw_low', 'roll_low', 'head_up', \n",
    "              'engage_0', 'engage_1', 'engage_2', 'engage_3', \n",
    "              'students_cnt', 'head_up_percentage', 'average_engage']\n",
    "    with open(filename[:-4] + '_get_date.csv', 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(header)\n",
    "        while frame_cnt < total_frames:\n",
    "            ret, im = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # 如果不是需要采样的帧，则跳过\n",
    "            # 若开启保存选项，则此处跳过的帧会在推理完成后回退处理并记录到保存的文件中\n",
    "            if frame_cnt % frames_interval != 0:\n",
    "                frame_cnt += 1\n",
    "                continue\n",
    "\n",
    "            if frame_cnt % (detect_faces_interval * math.ceil(fps)) == 0:\n",
    "                interval_begin_frame = frame_cnt\n",
    "                face_locations = get_face_location(im, detector)\n",
    "                numpy_video_dataset = [[] for _ in range(len(face_locations))]\n",
    "                head_pose_list = get_head_pose_list(im, face_locations, head_pose_model)\n",
    "                # for i, head_pose in enumerate(head_pose_list):\n",
    "                #     print(i, head_pose)\n",
    "\n",
    "            for i, [x1, y1, x2, y2] in enumerate(face_locations):\n",
    "                face_im = im[y1:y2, x1:x2, :]\n",
    "                face_im = cv2.resize(face_im, (224,224))\n",
    "                if len(face_locations) != len(numpy_video_dataset):\n",
    "                    print('frame_cnt:', frame_cnt)\n",
    "                    print(len(face_locations))\n",
    "                    print(len(numpy_video_dataset))\n",
    "                numpy_video_dataset[i].append(face_im)\n",
    "\n",
    "            frame_cnt += 1\n",
    "\n",
    "            if frame_cnt % 100 == 0:\n",
    "                print('cur_frame:', frame_cnt)\n",
    "\n",
    "            if check_data_already(numpy_video_dataset, frames_per_interval):\n",
    "                current_frame = frame_cnt\n",
    "\n",
    "                # 处理数据将其准备成可以输入模型的格式\n",
    "                my_dataset = frames_to_dataset(numpy_video_dataset)\n",
    "                my_dataloader = get_dataloader(None, my_dataset)\n",
    "\n",
    "                # 输入模型得到结果\n",
    "                results = get_model_output(model, my_dataloader)\n",
    "\n",
    "                # 统计这一段时间内的专注度分布情况\n",
    "                max_indexs = [top2_index_weighted_sum(result[0]) for result in results]\n",
    "                # engagements = [engagement_list_English[max_index] for max_index in max_indexs]\n",
    "                max_indexs = adjust_indexs_by_head_pose(max_indexs, head_pose_list)\n",
    "                engagements = max_indexs\n",
    "                # print(engagements)\n",
    "                # print(len(engagements))\n",
    "\n",
    "                # 开始进行数据统计，分别记录pitch、yaw、roll小于-45人数\n",
    "                # 抬头人数，总人数，专注度分别为0、1、2、3的人数以及平均专注度和抬头率\n",
    "                pitch_low, yaw_low, roll_low, head_up = get_pose_low(head_pose_list)\n",
    "                students_cnt = len(max_indexs)\n",
    "                engage_0, engage_1, engage_2, engage_3 = get_engage(max_indexs)\n",
    "                head_up_percentage = head_up / len(head_pose_list) * 100\n",
    "                average_engage = (engage_1 + engage_2 * 2 + engage_3 * 3) / students_cnt\n",
    "\n",
    "                # 开始进行csv的写入\n",
    "                begin_stamp = interval_begin_frame / fps\n",
    "                data_to_write = [begin_stamp, \n",
    "                                pitch_low, yaw_low, roll_low, head_up, \n",
    "                                engage_0, engage_1, engage_2, engage_3, \n",
    "                                students_cnt, head_up_percentage, average_engage]\n",
    "                writer.writerow(data_to_write)\n",
    "                print(filename[-22:], begin_stamp, 'finished')\n",
    "\n",
    "\n",
    "                # 如果需要处理视频的话在此处进行处理\n",
    "                if len(face_locations) == len(max_indexs) and save_avi:\n",
    "                    # 设置视频的帧数\n",
    "                    cap.set(cv2.CAP_PROP_POS_FRAMES, interval_begin_frame)\n",
    "                    # for i in range(interval_begin_frame, frame_cnt):\n",
    "                    for i in range(detect_faces_interval * math.ceil(fps)):\n",
    "                        ret, im = cap.read()\n",
    "                        im = mark_faces_by_index(im, face_locations, max_indexs)\n",
    "                        out.write(im)\n",
    "\n",
    "                # clear the data\n",
    "                numpy_video_dataset = []\n",
    "\n",
    "        if len(numpy_video_dataset) > 0 and not check_data_already(numpy_video_dataset, frames_per_interval):\n",
    "            for numpy_video in numpy_video_dataset:\n",
    "                while len(numpy_video) < frames_per_interval:\n",
    "                    numpy_video.append(np.zeros((224,224,3), dtype=np.uint8))\n",
    "\n",
    "        # 若有一段数据没有补齐到15帧序列长度，则在此补齐\n",
    "        if check_data_already(numpy_video_dataset, frames_per_interval):\n",
    "            # 处理数据将其准备成可以输入模型的格式\n",
    "            my_dataset = frames_to_dataset(numpy_video_dataset)\n",
    "            my_dataloader = get_dataloader(None, my_dataset)\n",
    "\n",
    "            # 输入模型得到结果\n",
    "            results = get_model_output(model, my_dataloader)\n",
    "\n",
    "            # 统计这一段时间内的专注度分布情况\n",
    "            max_indexs = [top2_index_weighted_sum(result[0]) for result in results]\n",
    "            # engagements = [engagement_list_English[max_index] for max_index in max_indexs]\n",
    "            max_indexs = adjust_indexs_by_head_pose(max_indexs, head_pose_list)\n",
    "            engagements = max_indexs\n",
    "            # print(engagements)\n",
    "\n",
    "            # 开始进行数据统计，分别记录pitch、yaw、roll小于-45人数\n",
    "            # 抬头人数，总人数，专注度分别为0、1、2、3的人数以及平均专注度和抬头率\n",
    "            pitch_low, yaw_low, roll_low, head_up = get_pose_low(head_pose_list)\n",
    "            students_cnt = len(max_indexs)\n",
    "            if students_cnt == 0:\n",
    "                students_cnt = -1\n",
    "            engage_0, engage_1, engage_2, engage_3 = get_engage(max_indexs)\n",
    "            head_up_percentage = head_up / len(head_pose_list) * 100\n",
    "            average_engage = (engage_1 + engage_2 * 2 + engage_3 * 3) / students_cnt\n",
    "\n",
    "            # 开始进行csv的写入\n",
    "            begin_stamp = interval_begin_frame / fps\n",
    "            data_to_write = [begin_stamp, \n",
    "                                pitch_low, yaw_low, roll_low, head_up, \n",
    "                                engage_0, engage_1, engage_2, engage_3, \n",
    "                                students_cnt, head_up_percentage, average_engage]\n",
    "            if students_cnt != -1:\n",
    "                writer.writerow(data_to_write)\n",
    "\n",
    "            # 如果需要处理视频的话在此处进行处理\n",
    "            if len(face_locations) == len(max_indexs) and save_avi:\n",
    "                # 设置视频的帧数\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, interval_begin_frame)\n",
    "                # for i in range(interval_begin_frame, frame_cnt):\n",
    "                for i in range(detect_faces_interval * math.ceil(fps)):\n",
    "                    ret, im = cap.read()\n",
    "                    im = mark_faces_by_index(im, face_locations, max_indexs)\n",
    "                    out.write(im)\n",
    "\n",
    "            # clear the data\n",
    "            numpy_video_dataset = []\n",
    "    if save_avi and out.isOpened():\n",
    "        out.release()\n",
    "\n",
    "    print(len(numpy_video_dataset))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 批量处理视频"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1]\n"
     ]
    }
   ],
   "source": [
    "l = [i for i in range(2)]\n",
    "print(l)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 将视频分段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /home/ubuntu/Videos/教学视频/example/A_example.mp4.\n",
      "MoviePy - Writing audio in A_exampleTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video /home/ubuntu/Videos/教学视频/example/A_example.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/ubuntu/Videos/教学视频/example/A_example.mp4\n"
     ]
    }
   ],
   "source": [
    "from moviepy.video.io.VideoFileClip import VideoFileClip\n",
    "\n",
    "# 读取视频文件\n",
    "video = VideoFileClip('/home/ubuntu/Videos/教学视频/example/A2021-20222021-2022-2+北京航空航天大学++教学班+大学计算机基础+孙青_学生_03.01_9.50_example.mp4')\n",
    "\n",
    "# 获取视频的总时长\n",
    "duration = video.duration\n",
    "\n",
    "# # 将视频分割成多个片段\n",
    "# start_times = [0, duration / 3, 2 * duration / 3]\n",
    "# end_times = [duration / 3, 2 * duration / 3, duration]\n",
    "# clips = [video.subclip(start, end) for start, end in zip(start_times, end_times)]\n",
    "\n",
    "# # 保存每个片段为不同的文件\n",
    "# for i, clip in enumerate(clips):\n",
    "#     clip.write_videofile(f'path/to/video_part_{i}.mp4')\n",
    "\n",
    "# 提取视频的前15秒\n",
    "clip = video.subclip(0, 15)\n",
    "\n",
    "# 保存提取出来的片段为文件\n",
    "clip.write_videofile('/home/ubuntu/Videos/教学视频/example/A_example.mp4', codec='libx264')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以下内容与主程序无关"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing efficientnet\n",
      "<class 'models.multimodalcnn.MultiModalCNN'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理音视频数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 依旧是不用处理音频"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# print(type(audio_feature_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-20222021-2022-2+北京航空航天大学++教学班+大学计算机基础+孙青_学生_03.22_14.50.mp4\n",
      "2021-20222021-2022-2+北京航空航天大学++教学班+大学计算机基础+孙青_学生_04.19_14.00.mp4\n",
      "2021-20222021-2022-2+北京航空航天大学++教学班+大学计算机基础+孙青_学生_03.08_14.50.mp4\n",
      "2021-20222021-2022-2+北京航空航天大学++教学班+大学计算机基础+孙青_学生_03.29_14.00.mp4\n",
      "2021-20222021-2022-2+北京航空航天大学++教学班+大学计算机基础+孙青_学生_04.26_14.00.mp4\n",
      "2021-20222021-2022-2+北京航空航天大学++教学班+大学计算机基础+孙青_学生_03.29_14.50.mp4\n",
      "2021-20222021-2022-2+北京航空航天大学++教学班+大学计算机基础+孙青_学生_03.08_14.00.mp4\n",
      "2021-20222021-2022-2+北京航空航天大学++教学班+大学计算机基础+孙青_学生_04.12_14.00.mp4\n",
      "2021-20222021-2022-2+北京航空航天大学++教学班+大学计算机基础+孙青_学生_03.01_14.00.mp4\n",
      "2021-20222021-2022-2+北京航空航天大学++教学班+大学计算机基础+孙青_学生_03.15_14.50.mp4\n",
      "2021-20222021-2022-2+北京航空航天大学++教学班+大学计算机基础+孙青_学生_04.19_14.50.mp4\n",
      "2021-20222021-2022-2+北京航空航天大学++教学班+大学计算机基础+孙青_学生_03.01_14.50.mp4\n",
      "2021-20222021-2022-2+北京航空航天大学++教学班+大学计算机基础+孙青_学生_03.15_14.00.mp4\n",
      "2021-20222021-2022-2+北京航空航天大学++教学班+大学计算机基础+孙青_学生_04.26_14.50.mp4\n",
      "2021-20222021-2022-2+北京航空航天大学++教学班+大学计算机基础+孙青_学生_04.12_14.50.mp4\n",
      "2021-20222021-2022-2+北京航空航天大学++教学班+大学计算机基础+孙青_学生_03.22_14.00.mp4\n",
      "2021-20222021-2022-2+北京航空航天大学++教学班+大学计算机基础+孙青_学生_03.15_10.40.mp4\n",
      "2021-20222021-2022-2+北京航空航天大学++教学班+大学计算机基础+孙青_学生_04.19_10.40.mp4\n",
      "2021-20222021-2022-2+北京航空航天大学++教学班+大学计算机基础+孙青_学生_03.29_9.50.mp4\n",
      "2021-20222021-2022-2+北京航空航天大学++教学班+大学计算机基础+孙青_学生_04.12_10.40.mp4\n",
      "2021-20222021-2022-2+北京航空航天大学++教学班+大学计算机基础+孙青_学生_03.22_9.50.mp4\n",
      "2021-20222021-2022-2+北京航空航天大学++教学班+大学计算机基础+孙青_学生_03.08_10.40.mp4\n",
      "2021-20222021-2022-2+北京航空航天大学++教学班+大学计算机基础+孙青_学生_04.12_9.50.mp4\n",
      "2021-20222021-2022-2+北京航空航天大学++教学班+大学计算机基础+孙青_学生_03.29_10.40.mp4\n",
      "2021-20222021-2022-2+北京航空航天大学++教学班+大学计算机基础+孙青_学生_04.19_9.50.mp4\n",
      "2021-20222021-2022-2+北京航空航天大学++教学班+大学计算机基础+孙青_学生_04.26_10.40.mp4\n",
      "2021-20222021-2022-2+北京航空航天大学++教学班+大学计算机基础+孙青_学生_03.01_9.50.mp4\n",
      "2021-20222021-2022-2+北京航空航天大学++教学班+大学计算机基础+孙青_学生_03.22_10.40.mp4\n",
      "2021-20222021-2022-2+北京航空航天大学++教学班+大学计算机基础+孙青_学生_03.08_9.50.mp4\n",
      "2021-20222021-2022-2+北京航空航天大学++教学班+大学计算机基础+孙青_学生_03.15_9.50.mp4\n",
      "2021-20222021-2022-2+北京航空航天大学++教学班+大学计算机基础+孙青_学生_04.26_9.50.mp4\n",
      "2021-20222021-2022-2+北京航空航天大学++教学班+大学计算机基础+孙青_学生_03.01_10.40.mp4\n"
     ]
    }
   ],
   "source": [
    "parent = '/home/ubuntu/Videos/教学视频/student_processed'\n",
    "for foldername in os.listdir(parent):\n",
    "    for filename in os.listdir(os.path.join(parent, foldername)):\n",
    "        print(filename)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理原视频，提取人脸位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fearful', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise', 'surprise']\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "# 提取识别到的表情\n",
    "expression_list_Chinese = [\"中性\", \"平静\", \"快乐\", \"悲伤\", \"愤怒\", \"恐惧\", \"厌恶\", \"惊讶\"]\n",
    "expression_list_English = [\"neutral\", \"calm\", \"happy\", \"sad\", \"angry\", \"fearful\", \"disgust\", \"surprise\"]\n",
    "max_indexs = [torch.argmax(result[0]) for result in results]\n",
    "expressions = [expression_list_English[max_index] for max_index in max_indexs]\n",
    "print(expressions)\n",
    "print(len(expressions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data.\n",
      "out of index\n",
      "out of index\n",
      "out of index\n",
      "out of index\n"
     ]
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存视频"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上面的代码已经保存了视频了\n",
    "# 我能否做出一个实时效果的展示视频呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建一个2x3的tensor\n",
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# 获取tensor的底层数据\n",
    "data = x.data.numpy()\n",
    "\n",
    "# 打印底层数据\n",
    "print(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 热力图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# 读取人脸图像\n",
    "img = cv2.imread('/home/ubuntu/work_space/Draw_pictures_by_python/test_imgs/01image00023.jpg')\n",
    "\n",
    "# 确定每个人脸的得分\n",
    "scores = [0.2, 0.8, 0.6, 0.4, 0.9]\n",
    "scores = np.random.rand(img.shape[0], img.shape[1], img.shape[2])\n",
    "scores = np.random.rand(img.shape[0], img.shape[1])\n",
    "\n",
    "# 将每个人脸的得分映射到颜色空间\n",
    "color_map = cv2.applyColorMap(np.uint8(scores * 255), cv2.COLORMAP_JET)\n",
    "\n",
    "# 将映射后的颜色图像与原始图像进行叠加\n",
    "alpha = 0.5\n",
    "dst = cv2.addWeighted(img, alpha, color_map, 1 - alpha, 0)\n",
    "\n",
    "# 显示结果\n",
    "cv2.imwrite('result.jpg', dst)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# 创建一个空白图像\n",
    "img = np.zeros((512, 512), np.uint8)\n",
    "\n",
    "# 定义圆形区域的参数\n",
    "circles = [(100, 100, 50), (200, 200, 30), (300, 300, 70)]\n",
    "\n",
    "# 遍历所有圆形区域，并将其赋值为255\n",
    "for circle in circles:\n",
    "    cv2.circle(img, (circle[0], circle[1]), circle[2], 255, -1)\n",
    "\n",
    "# 显示图像\n",
    "cv2.imwrite('result1.jpg', img)\n",
    "# cv2.imshow(\"Image\", img)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# 创建一个空白图像\n",
    "img = np.zeros((512, 512), np.uint8)\n",
    "\n",
    "# 定义圆形区域的参数\n",
    "circles = [(100, 100, 50), (200, 200, 30), (300, 300, 70)]\n",
    "\n",
    "# 遍历所有圆形区域，并将其赋值为指定的值\n",
    "for circle in circles:\n",
    "    # 计算圆心到图像中心的距离\n",
    "    distance = np.sqrt((circle[0] - 256) ** 2 + (circle[1] - 256) ** 2)\n",
    "    # 计算圆形区域的值\n",
    "    value = 255 - int(distance / 2)\n",
    "    # 使用cv2.circle()函数绘制圆形\n",
    "    cv2.circle(img, (circle[0], circle[1]), circle[2], value, -1)\n",
    "\n",
    "# 显示图像\n",
    "cv2.imwrite('result2.jpg', img)\n",
    "# cv2.imshow(\"Image\", img)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal-emotion-recognition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
